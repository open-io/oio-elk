Docker ELK + FILEBEAT+ OPENIO/SDS    
----------------------

> les diiférentes versions des outils utilisés:  
- ELK : version 6.2.4   
- Filebeat : Version 6.2.4  
- Openio/sds: version 17.04


#### Requirement
 [Installer docker](https://docs.docker.com/install/)   
 [Installer docker-compose ](https://docs.docker.com/compose/install/)

### Setup

#### Clone the repository

 ```
 $ git clone https://github.com/papebadiane/docker-elkf && cd docker-elkf
 ```

#### increase host's virtual memory
  ```
  $ sudo sysctl -w vm.max_map_count=262144
  ```
### Run containers
 * if you just want  to run  only the container ELK, you need to run the following command
 ```
 $ docker-compose up -d --build elk
 ```
 * if you just want run all containers (elk, openio-sds, Filebeat) for a local test, you need to run the following command
 ```
 $ docker-compose up -d --build
 ```

After the services ( elasticsearch, kibana and logstash) are up, you need the following commands to load security configuration

```
$ docker exec docker-elkf_elk bash
# /etc/elasticsearch/bin/sgadmin_install.sh  
```

### Logging to kibana

Go to http://[IP_HOST]:5601 and

Login: admin   
Password: admin
__________________________________________________________________________________
## Configuration


### Search Guard
 Search guard is an Elasticsearch plugin that offers encryption, authentication, and authorization, we  use it to secure Elasticsearch cluster and kibana's interface.
##### Configuration file
 The configuration consists of the following files and they are located at `/etc/elasticsearch/sgconfig`.

 * `sg_config.yml` - configure authenticators and authorisation backends.
 * `sg_roles.yml` - define roles and the associated permissions.
 * `sg_roles_mapping.yml` - map backend roles, hosts and users to roles.
 * `sg_internal_users.yml` - stores users, roles and hashed passwords (hash with hash.sh) in the internal user database.
 * `sg_action_groups.yml` - define named permission groups.


###### How to generate certificates using PKI
The script to execute is `example.sh`, located in the folder `/etc/elasticsearch/pki-scripts`, it generate

###### Customizing the certificates

if you need to customize the certificates generated by the PKI scripts, the following files define the configuration of the Root CA and signing CA:
```
/etc/elasticsearch/pki-scripts/etc/root-ca.conf
/etc/elasticsearch/pki-scripts/etc/signing-ca.conf
```
 You need to change the DN of two files depend on your own company, like this:

```
[ ca_dn ]
0.domainComponent       = "com"
1.domainComponent       = "example"
organizationName        = "Example Com Inc."
organizationalUnitName  = "Example Com Inc. Root CA"
commonName              = "Example Com Inc. Root CA"
```
###### Command to run  to generate new certificates

```
# /etc/elasticsearch/pki-scripts/example.sh
```
After that you run the command differents files are generated but we interessed at three of them:

* `node-0-keystore.jks`
* `trustore.jks`
* `kirk-keysote.jks`

Copy these files to directory `/etc/elasticsearch/sgconfig`


##### How to Change the password of user

Run the following command

 ```
# /opt/elastic/plugins/search-guard/tools/hash.sh -p <new password>
```

Open the file `$/etc/elasticsearch/sgconfig/sg_roles.yml`and the replace the hash of specific user with the new hash

```
<user_login>:
  hash: < new hash >

```

### Elastic curator

For the management of data retention we use Elastic curator.
Elastic curator allow us to delete index, to create or to restore snapshot according to certain characters.

##### Configuration file
The configuration file contains client connection and settings for logging.

the location of the  configuration file is `/opt/elasticsearch/curator/curator.yml`, it looks like this

 ```
 client:
   hosts:
     - 127.0.0.1
   port: 9200
   url_prefix:
   use_ssl: False
   certificate:
   client_cert:
   client_key:
   ssl_no_validate: False
   http_auth: "curator:curator"
   timeout: 30
   master_only: False

 logging:
   loglevel: INFO
   logfile:
   logformat: default
   blacklist: ['elasticsearch', 'urllib3']

```

##### Action file
The action file contains the tasks which Curator can perform on our index.
These tasks may be to create index, to delete index, to create a snapshot etc.

This is an example of  the action delete all index matching with the pattern `òio-*` and older 30 days

 ```
 actions:
   1:
     action: delete_indices
     description: >-
       Delete indices older than 30 days based on oio- prefixed indices
     options:
       ignore_empty_list: True
       disable_action: True
     filters:
     - filtertype: pattern
       kind: prefix
       value: oio-
     - filtertype: age
       source: creation_date
       unit: days
       unit_count: 30
       direction: older

 ```

In our case, we created 3 files action for the management of data retention, the located into `/opt/elasticsearch/curator`:

* `delete_indices.yml` which delete indices
* `snapshot.yml` which create a snapshot
* `restore_snapshot.yml` which permit to restore a specific snapshot

##### Run a curator
```
# /usr/bin/curator /opt/elasticsearch/curato/delete_indices.yml --config /opt/elasticsearch/curato/curator.yml

```

##### How to change the hour to run  curator scripts

The tasks to create snapshot and to delete index are automatize with crontab, We can change the hour to run curator, you just need to run the command `crontab -e ` and modify the parameter
```
00 02 * * * /usr/bin/curator /opt/elasticsearch/curator/snapshot.yml --config /opt/elasticsearch/curator/curator.yml
02 02 * * * /usr/bin/curator /opt/elasticsearch/curator/delete_indices.yml --config /opt/elasticsearch/curator/curator.yml
```

##### How to  Create a snapshot

if you want create or restore a snapshot, firstly you need to run of these following commands before that you run the curator script

* For Filesystem backup
```
$ curl -XPUT 'localhost:9200/_snapshot/oio_logs_backup' -H 'Content-Type: application/json' -d '{ "type": "fs", "settings": {"location": "/opt/repo_snapshots","compress": true}}'
```
*  For S3  backup
```
$ curl -XPUT 'localhost:9200/_snapshot/s3_repository' -H 'Content-Type: application/json' -d'
{
  "type": "s3",
  "settings": {
    "bucket": "bucket_name",
    "server_side_encryption": false,
    "client": "default"
  }
}'
```
